{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb5ed5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77f55c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Indices:\n",
      " [   0    3    5    6   10   11   13 3299 3300 3301]\n",
      "Selected Features:\n",
      " Index(['Age', 'Customer_Support_Interactions', 'Customer_Satisfaction',\n",
      "       'Purchase_Frequency', 'Lifetime_Value', 'Average_Order_Value',\n",
      "       'Number_of_Product_Categories_Purchased',\n",
      "       'Loyalty_Program_Participation_Inactive',\n",
      "       'Engagement_with_Promotions_Low', 'Engagement_with_Promotions_Medium'],\n",
      "      dtype='object')\n",
      "\n",
      "Random Forest\n",
      "Confusion Matrix:\n",
      " [[17143    44]\n",
      " [    0  6589]]\n",
      "Accuracy: 0.9981493943472409\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      1.00      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       1.00      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "Logistic Regression\n",
      "Confusion Matrix:\n",
      " [[17112    75]\n",
      " [    0  6589]]\n",
      "Accuracy: 0.9968455585464334\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      0.99      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       0.99      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "Decision Tree\n",
      "Confusion Matrix:\n",
      " [[17147    40]\n",
      " [   26  6563]]\n",
      "Accuracy: 0.9972240915208613\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      0.99      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       1.00      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "K-Nearest Neighbors\n",
      "Confusion Matrix:\n",
      " [[16973   214]\n",
      " [    4  6585]]\n",
      "Accuracy: 0.9908310901749664\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     17187\n",
      "           1       0.97      1.00      0.98      6589\n",
      "\n",
      "    accuracy                           0.99     23776\n",
      "   macro avg       0.98      0.99      0.99     23776\n",
      "weighted avg       0.99      0.99      0.99     23776\n",
      "\n",
      "\n",
      "Support Vector Machine\n",
      "Confusion Matrix:\n",
      " [[17102    85]\n",
      " [    0  6589]]\n",
      "Accuracy: 0.9964249663526245\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      0.99      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       0.99      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "Gradient Boosting\n",
      "Confusion Matrix:\n",
      " [[17137    50]\n",
      " [    0  6589]]\n",
      "Accuracy: 0.9978970390309556\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      1.00      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       1.00      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "Naive Bayes\n",
      "Confusion Matrix:\n",
      " [[15843  1344]\n",
      " [    0  6589]]\n",
      "Accuracy: 0.9434724091520862\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96     17187\n",
      "           1       0.83      1.00      0.91      6589\n",
      "\n",
      "    accuracy                           0.94     23776\n",
      "   macro avg       0.92      0.96      0.93     23776\n",
      "weighted avg       0.95      0.94      0.94     23776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"cleaned_df_luxottica_churn_updated_0108.csv\", index_col=None)\n",
    "\n",
    "# Create a copy of the dataset for transformation\n",
    "dataset_transformed = pd.get_dummies(dataset, drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = dataset_transformed.drop('Churn_Yes', axis=1)\n",
    "y = dataset_transformed['Churn_Yes']\n",
    "\n",
    "# Initialize the SelectKBest with f_classif\n",
    "select_k_best = SelectKBest(score_func=f_classif, k=10)\n",
    "X_selected = select_k_best.fit_transform(X, y)\n",
    "\n",
    "# Get the selected feature indices and names\n",
    "selected_features_indices = select_k_best.get_support(indices=True)\n",
    "selected_features = X.columns[selected_features_indices]\n",
    "print(\"Selected Feature Indices:\\n\", selected_features_indices)\n",
    "print(\"Selected Features:\\n\", selected_features)\n",
    "\n",
    "# Create the final feature and target datasets with selected features\n",
    "X_final = X[selected_features]\n",
    "y_final = y\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.25, random_state=0)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Dictionary to store models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=0),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=0),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=0),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machine\": SVC(random_state=0),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=0),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{model_name}\")\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Display the results\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50bb1a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Cross-Validation Accuracy: 0.9981 (+/- 0.0003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AB92922\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\AB92922\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\AB92922\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\AB92922\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\AB92922\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Cross-Validation Accuracy: 0.9682 (+/- 0.0013)\n",
      "Decision Tree - Cross-Validation Accuracy: 0.9965 (+/- 0.0003)\n",
      "K-Nearest Neighbors - Cross-Validation Accuracy: 0.7520 (+/- 0.0007)\n",
      "Support Vector Machine - Cross-Validation Accuracy: 0.7361 (+/- 0.0010)\n",
      "Gradient Boosting - Cross-Validation Accuracy: 0.9980 (+/- 0.0003)\n",
      "Naive Bayes - Cross-Validation Accuracy: 0.9836 (+/- 0.0004)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluate model using cross-validation\n",
    "for model_name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_final, y_final, cv=5)\n",
    "    print(f\"{model_name} - Cross-Validation Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66b51b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Indices:\n",
      " [   0    3    5    6   10   11   13 3299 3300 3301]\n",
      "Selected Features:\n",
      " Index(['Age', 'Customer_Support_Interactions', 'Customer_Satisfaction',\n",
      "       'Purchase_Frequency', 'Lifetime_Value', 'Average_Order_Value',\n",
      "       'Number_of_Product_Categories_Purchased',\n",
      "       'Loyalty_Program_Participation_Inactive',\n",
      "       'Engagement_with_Promotions_Low', 'Engagement_with_Promotions_Medium'],\n",
      "      dtype='object')\n",
      "Random Forest - Training Accuracy: 1.0000, Testing Accuracy: 0.9981\n",
      "Confusion Matrix:\n",
      "[[17143    44]\n",
      " [    0  6589]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      1.00      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       1.00      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "Logistic Regression - Training Accuracy: 0.9975, Testing Accuracy: 0.9974\n",
      "Confusion Matrix:\n",
      "[[17126    61]\n",
      " [    0  6589]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      1.00      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       1.00      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "Decision Tree - Training Accuracy: 1.0000, Testing Accuracy: 0.9974\n",
      "Confusion Matrix:\n",
      "[[17148    39]\n",
      " [   23  6566]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      1.00      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       1.00      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "K-Nearest Neighbors - Training Accuracy: 0.8358, Testing Accuracy: 0.7488\n",
      "Confusion Matrix:\n",
      "[[15463  1724]\n",
      " [ 4248  2341]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.84     17187\n",
      "           1       0.58      0.36      0.44      6589\n",
      "\n",
      "    accuracy                           0.75     23776\n",
      "   macro avg       0.68      0.63      0.64     23776\n",
      "weighted avg       0.73      0.75      0.73     23776\n",
      "\n",
      "\n",
      "Support Vector Machine - Training Accuracy: 0.7358, Testing Accuracy: 0.7369\n",
      "Confusion Matrix:\n",
      "[[17034   153]\n",
      " [ 6102   487]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.99      0.84     17187\n",
      "           1       0.76      0.07      0.13      6589\n",
      "\n",
      "    accuracy                           0.74     23776\n",
      "   macro avg       0.75      0.53      0.49     23776\n",
      "weighted avg       0.74      0.74      0.65     23776\n",
      "\n",
      "\n",
      "Gradient Boosting - Training Accuracy: 0.9982, Testing Accuracy: 0.9979\n",
      "Confusion Matrix:\n",
      "[[17138    49]\n",
      " [    0  6589]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17187\n",
      "           1       0.99      1.00      1.00      6589\n",
      "\n",
      "    accuracy                           1.00     23776\n",
      "   macro avg       1.00      1.00      1.00     23776\n",
      "weighted avg       1.00      1.00      1.00     23776\n",
      "\n",
      "\n",
      "Naive Bayes - Training Accuracy: 0.9839, Testing Accuracy: 0.9831\n",
      "Confusion Matrix:\n",
      "[[16786   401]\n",
      " [    0  6589]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     17187\n",
      "           1       0.94      1.00      0.97      6589\n",
      "\n",
      "    accuracy                           0.98     23776\n",
      "   macro avg       0.97      0.99      0.98     23776\n",
      "weighted avg       0.98      0.98      0.98     23776\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"cleaned_df_luxottica_churn_updated_0108.csv\", index_col=None)\n",
    "\n",
    "# Create a copy of the dataset for transformation\n",
    "dataset_transformed = pd.get_dummies(dataset, drop_first=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = dataset_transformed.drop('Churn_Yes', axis=1)\n",
    "y = dataset_transformed['Churn_Yes']\n",
    "\n",
    "# Initialize the SelectKBest with f_classif\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "select_k_best = SelectKBest(score_func=f_classif, k=10)\n",
    "X_selected = select_k_best.fit_transform(X, y)\n",
    "\n",
    "# Get the selected feature indices and names\n",
    "selected_features_indices = select_k_best.get_support(indices=True)\n",
    "selected_features = X.columns[selected_features_indices]\n",
    "print(\"Selected Feature Indices:\\n\", selected_features_indices)\n",
    "print(\"Selected Features:\\n\", selected_features)\n",
    "\n",
    "# Create the final feature and target datasets with selected features\n",
    "X_final = X[selected_features]\n",
    "y_final = y\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.25, random_state=0)\n",
    "\n",
    "# Dictionary of classifiers\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=0),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=0, max_iter=10000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=0),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machine\": SVC(random_state=0),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=0),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on the training set\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Evaluate on the testing set\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"Training Accuracy\": train_accuracy,\n",
    "        \"Testing Accuracy\": test_accuracy,\n",
    "        \"Confusion Matrix\": confusion_matrix(y_test, y_test_pred),\n",
    "        \"Classification Report\": classification_report(y_test, y_test_pred)\n",
    "    }\n",
    "    print(f\"{name} - Training Accuracy: {train_accuracy:.4f}, Testing Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{results[name]['Confusion Matrix']}\")\n",
    "    print(f\"Classification Report:\\n{results[name]['Classification Report']}\\n\")\n",
    "\n",
    "# You can also store or print the results as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1587ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
